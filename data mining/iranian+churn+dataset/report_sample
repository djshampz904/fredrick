1. Thesis
1.1: Project Overview
1.2: Project Objectives
1.3: Project Environmental Configuration
1.4: Acknowledgements
2. Data Set Description
2.1: Dataset Overview
2.2: Gross Dataset Attributes
3. Pre-Experiment Preparation
3.1: Required Pre-Processing for Classification Algorithms
3.1.1: File Conversion
3.1.2: Preprocessing
3.1.3: Attribute Selection Methods
3.1.5: Algorithm Selection
3.5: K-Nearest Neighbour Algorithm
3.5.1: Algorithm Description
3.6: Support Vector Machine Algorithm
3.6.1: Algorithm Description
3.7: Decision Tree Algorithm1,
3.6.1: Algorithm Description
4. Attribute Selection Method Groupings
4.1: Gross Data Set
4.2: Recommended Data Set Attributes
4.3: Weka Optimized Data Set
5. Discussion Preface
5.1 Gross Data Set Results
5.2 Recommended Data Set Results
5.3 Weka Optimised Data Set Results
5.4 Overall Analysis of Experiment
6. Appendix
Appendix 1: Gross Data Set Attributes
Appendix 2: Recommended Data Set Attributes
Appendix 3: Weka Optimized Data Se


sample data set
customerID,gender,SeniorCitizen,Partner,Dependents,tenure,PhoneService,MultipleLines,InternetService,OnlineSecurity,OnlineBackup,DeviceProtection,TechSupport,StreamingTV,StreamingMovies,Contract,PaperlessBilling,PaymentMethod,MonthlyCharges,TotalCharges,Churn
7590-VHVEG,Female,0,Yes,No,1,No,No phone service,DSL,No,Yes,No,No,No,No,Month-to-month,Yes,Electronic check,29.85,29.85,No
5575-GNVDE,Male,0,No,No,34,Yes,No,DSL,Yes,No,Yes,No,No,No,One year,No,Mailed check,56.95,1889.5,No
3668-QPYBK,Male,0,No,No,2,Yes,No,DSL,Yes,Yes,No,No,No,No,Month-to-month,Yes,Mailed check,53.85,108.15,Yes
7795-CFOCW,Male,0,No,No,45,No,No phone service,DSL,Yes,No,Yes,Yes,No,No,One year,No,Bank transfer (automatic),42.3,1840.75,No
9237-HQITU,Female,0,No,No,2,Yes,No,Fiber optic,No,No,No,No,No,No,Month-to-month,Yes,Electronic check,70.7,151.65,Yes
9305-CDSKC,Female,0,No,No,8,Yes,Yes,Fiber optic,No,No,Yes,No,Yes,Yes,Month-to-month,Yes,Electronic check,99.65,820.5,Yes
1452-KIOVK,Male,0,No,Yes,22,Yes,Yes,Fiber optic,No,Yes,No,No,Yes,No,Month-to-month,Yes,Credit card (automatic),89.1,1949.4,No
6713-OKOMC,Female,0,No,No,10,No,No phone service,DSL,Yes,No,No,No,No,No,Month-to-month,No,Mailed check,29.75,301.9,No
7892-POOKP,Female,0,Yes,No,28,Yes,Yes,Fiber optic,No,No,Yes,Yes,Yes,Yes,Month-to-month,Yes,Electronic check,104.8,3046.05,Yes
6388-TABGU,Male,0,No,Yes,62,Yes,No,DSL,Yes,Yes,No,No,No,No,One year,No,Bank transfer (automatic),56.15,3487.95,No
9763-GRSKD,Male,0,Yes,Yes,13,Yes,No,DSL,Yes,No,No,No,No,No,Month-to-month,Yes,Mailed check,49.95,587.45,No
7469-LKBCI,Male,0,No,No,16,Yes,No,No,No internet service,No internet service,No internet service,No internet service,No internet service,No internet service,Two year,No,Credit card (automatic),18.95,326.8,No

#%%
import pandas as pd
#%%
# Finally, merge the result with security_data
customer_churn_data_combined = pd.read_csv('WA_Fn-UseC_-Telco-Customer-Churn.csv')
#%%
customer_churn_data_combined.head()
#%%
customer_churn_data_combined.head()
#%% md
<!DOCTYPE html>
<html>
<head>
    <title>Column Value Transformation</title>
</head>
<body>
    <h1>Transforming Column Values</h1>
    <p>Handling the categorical data in the columns contract and payment method.</p>
</body>
</html>
#%%
customer_billing = pd.get_dummies(customer_churn_data_combined, columns=['Contract', 'PaymentMethod'], drop_first=True)
#%%
customer_billing.head()
#%%
customer_billing = pd.get_dummies(customer_billing, columns=["Churn"], drop_first=True)
#%%
customer_billing.head()
#%% md
<!DOCTYPE html>
<html>
<head>
    <title>Data Preprocessing dummies</title>
</head>
<body>
    <h1>Data Preprocessing for Machine Learning</h1>
    <p>In the current project, we are dealing with a dataset that contains categorical data. Categorical data is a type of data that can take on one of a limited number of categories. For example, in our dataset, the 'contract' and 'paymentmethod' columns contain various categories.</p>
    <p>Most machine learning algorithms require numerical input and output variables. So, we need to convert these categorical data into a numerical format. One common technique for this conversion is called one-hot encoding.</p>
    <p>In pandas, the <code>get_dummies</code> function is used to convert categorical variable(s) into dummy/indicator variables. For each unique value in the categorical column, it creates a new column that represents whether the record has that value. If the record has that value, it will be 1, otherwise it will be 0.</p>
    <p>Here's how we can apply this in our project:</p>
    <pre>
    <code>
    customer_billing = pd.get_dummies(customer_billing, columns=["partner", "dependants", "paperlessbilling"], drop_first=True)
    </code>
    </pre>
    <p>The <code>drop_first=True</code> argument is used to avoid the dummy variable trap, which is a scenario in which the independent variables are multicollinear.</p>
</body>
</html>
#%%
customer_billing = pd.get_dummies(customer_billing, columns=["Partner", "Dependents", "PaperlessBilling"], drop_first=True)
#%%
customer_billing.sample(5)
#%% md
<!DOCTYPE html>
<html>
<head>
    <title>Conducting exploratory analysis</title>
</head>
<body>
    <h1>Exploratory Data Analysis</h1>
    <p>Exploratory data analysis (EDA) is an approach to analyzing datasets to summarize their main characteristics, often with visual methods. It helps us to understand the data, discover patterns, spot anomalies, and check assumptions.</p>
    <p>Here are some common techniques used in EDA:</p>
    <ul>
        <li>Descriptive statistics: Summarizing the data using its key characteristics, such as the mean, median, mode, standard deviation, and range.</li>
        <li>Grouping data: Using aggregation and grouping methods to understand the relationships between variables.</li>
        <li>Data visualization: Creating charts, plots, and graphs to visually represent the data.</li>
        <li>Correlation analysis: Examining the relationships between variables to identify patterns and trends.</li>
    </ul>
    <p>Let's conduct some exploratory analysis on our dataset to understand the relationships between different variables.</p>
#%%
import matplotlib.pyplot as plt
#%%
#total customers
customer_count = len(customer_billing['Churn_Yes'])
churned = len(customer_billing[customer_billing['Churn_Yes'] == 1])
not_churned = len(customer_billing[customer_billing['Churn_Yes'] == 0])
#%%
print ("churned : {},\n not_churned {} \n total customers: {}".format(churned, not_churned, customer_count))
#%%
category = ['Churned', 'Not Churned']
values = [(churned * 100 / customer_count), (not_churned * 100 / customer_count)]
bars = plt.bar(category, values)
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval + 1, round(yval, 2), ha='center', va='bottom')

plt.title('Customer Churn')
plt.show()
#%%
# summary of table data
customer_billing.describe()
#%%
telco = customer_churn_data_combined.copy()
#%%
telco.head()
#%%
#display all telco columns
telco.columns
#%%
telco.head()
#%%
telco.TotalCharges = pd.to_numeric(telco.TotalCharges, errors='coerce')
telco.isnull().sum()
#%%
telco.loc[telco.TotalCharges.isnull() == True]
#%%
telco.dropna(how='any', inplace=True)
#%%
# We will create a category group for tenure check the max tenure
print(telco['tenure'].max())
#%%
labels = ["{0} - {1}".format(i, i + 11) for i in range(1, 72, 12)]
telco['tenure_group'] = pd.cut(telco.tenure, range(1, 80, 12), right=False, labels=labels)
#%%
print(labels)
#%%
telco['tenure_group'].value_counts()
#%%
# Remove columns we dont need for the model
telco.drop(columns=['customerID', 'tenure'], axis=1, inplace=True)
#%%
telco.head()
#%%
import seaborn as sns
for i, predictor in enumerate(telco.drop(columns=['Churn', 'TotalCharges', 'MonthlyCharges'])):
    plt.figure(i)
    sns.countplot(data=telco, x=predictor, hue='Churn')
#%%
# Conver the churn to binary with 1 as yes and 0 as no
import numpy as np
telco['Churn'] = np.where(telco.Churn == 'Yes', 1, 0)
#%%
telco.head()
#%%
# convert categorical data to using dummy library
telco_dummies = pd.get_dummies(telco)
telco_dummies.head()
#%%
# motnly charges and total relationship
sns.lmplot(data=telco, x='MonthlyCharges', y='TotalCharges', fit_reg=False)
#%%
Mth = sns.kdeplot(telco_dummies.MonthlyCharges[(telco_dummies["Churn"] == 0) ],
                color="Red", fill = True)
Mth = sns.kdeplot(telco_dummies.MonthlyCharges[(telco_dummies["Churn"] == 1) ],
                ax =Mth, color="Blue", fill= True)
Mth.legend(["No Churn","Churn"],loc='upper right')
Mth.set_ylabel('Density')
Mth.set_xlabel('Monthly Charges')
Mth.set_title('Monthly charges by churn')
#%%
Tot = sns.kdeplot(telco_dummies.TotalCharges[(telco_dummies["Churn"] == 0) ],
                color="Red", fill = True)
Tot = sns.kdeplot(telco_dummies.TotalCharges[(telco_dummies["Churn"] == 1) ],
                ax =Tot, color="Blue", fill= True)
Tot.legend(["No Churn","Churn"],loc='upper right')
Tot.set_ylabel('Density')
Tot.set_xlabel('Total Charges')
Tot.set_title('Total charges by churn')
#%%
# find the correlation of churn with other variables
plt.figure(figsize=(20,8))
telco_dummies.corr()['Churn'].sort_values(ascending = False).plot(kind='bar')
#%%
new_df1_target0=telco.loc[telco["Churn"]==0]
new_df1_target1=telco.loc[telco["Churn"]==1]
#%%
def uniplot(df,col,title,hue =None):

    sns.set_style('whitegrid')
    sns.set_context('talk')
    plt.rcParams["axes.labelsize"] = 20
    plt.rcParams['axes.titlesize'] = 22
    plt.rcParams['axes.titlepad'] = 30

    temp = pd.Series(data = hue)
    fig, ax = plt.subplots()
    width = len(df[col].unique()) + 7 + 4*len(temp.unique())
    fig.set_size_inches(width , 8)
    plt.xticks(rotation=45)
    # plt.yscale('log')  # Commented out to set y-axis scale to linear
    plt.title(title)
    ax = sns.countplot(data = df, x= col, order=df[col].value_counts().index,hue = hue,palette='bright')

    plt.show()
#%%
uniplot(new_df1_target1,col='Partner',title='Distribution of Gender for Churned Customers',hue='gender')
#%%
uniplot(new_df1_target0,col='Partner',title='Distribution of Gender for Non Churned Customers',hue='gender')
#%%
uniplot(new_df1_target1,col='PaymentMethod',title='Distribution of PaymentMethod for Churned Customers',hue='gender')
#%%
uniplot(new_df1_target1,col='Contract',title='Distribution of Contract for Churned Customers',hue='gender')
#%%
uniplot(new_df1_target1,col='TechSupport',title='Distribution of TechSupport for Churned Customers',hue='gender')

#%%
uniplot(new_df1_target1,col='SeniorCitizen',title='Distribution of SeniorCitizen for Churned Customers',hue='gender')
#%%
telco_dummies.to_csv('tel_churn.csv')
#%%

#%%
from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn.metrics import recall_score
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.tree import DecisionTreeClassifier
from imblearn.combine import SMOTEENN
#%%
df = pd.read_csv('tel_churn.csv')
df.head()
#%%
df  = df.drop("Unnamed: 0", axis=1)
#%%
# We create our feature columns
x = df.drop('Churn', axis=1)
x.head()
#%%
# create our target column
y = df['Churn']
y.head()
#%%
# splitting data set
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2)
#%%
# create our model that will use the DecisionTreeClassifier because we are dealing with a classification problem
model_dt=DecisionTreeClassifier(criterion = "gini",random_state = 100,max_depth=6, min_samples_leaf=8)
#%%
# fit our model
model_dt.fit(x_train, y_train)
#%%
# predict the model
y_pred = model_dt.predict(x_test)
y_pred
#%%
# check our model accuracy
model_dt.score(x_test, y_test)
#%%
print(classification_report(y_test, y_pred, labels=[0, 1]))
#%%
# our accuracy is low so we will use SMOTETomek to balance our data
smt = SMOTEENN()
X_resampled, y_resampled = smt.fit_resample(x, y)
#%%
xr_train,xr_test,yr_train,yr_test=train_test_split(X_resampled,y_resampled,test_size=0.2)
#%%
model_dt_smote=DecisionTreeClassifier(criterion = "gini",random_state = 100,max_depth=6, min_samples_leaf=8)
#%%
model_dt_smote.fit(xr_train,yr_train)
yr_predict = model_dt_smote.predict(xr_test)
model_score_r = model_dt_smote.score(xr_test, yr_test)
print(model_score_r)
print(metrics.classification_report(yr_test, yr_predict))
#%%
# as we can see our model accuracy has increased
print(metrics.confusion_matrix(yr_test, yr_predict))
#%%
# we will not try using Random Forest to see if we can get a better accuracy
from sklearn.ensemble import RandomForestClassifier
model_rf=RandomForestClassifier(n_estimators=100, criterion='gini', random_state = 100,max_depth=6, min_samples_leaf=8)
#%%
model_rf.fit(x_train,y_train)
#%%
y_pred=model_rf.predict(x_test)
#%%
model_rf.score(x_test,y_test)
#%%
print(classification_report(y_test, y_pred, labels=[0,1]))
#%%
sm = SMOTEENN()
X_resampled1, y_resampled1 = sm.fit_resample(x, y)
#%%
xr_train1,xr_test1,yr_train1,yr_test1=train_test_split(X_resampled1, y_resampled1,test_size=0.2)
#%%
model_rf_smote=RandomForestClassifier(n_estimators=100, criterion='gini', random_state = 100,max_depth=6, min_samples_leaf=8)
#%%
model_rf_smote.fit(xr_train1,yr_train1)
#%%
yr_predict1 = model_rf_smote.predict(xr_test1)
#%%
model_score_r1 = model_rf_smote.score(xr_test1, yr_test1)
print(model_score_r1)
#%%
print(confusion_matrix(yr_test1, yr_predict1))
#%%
from sklearn.decomposition import PCA
pca = PCA(0.9)
xr_train_pca = pca.fit_transform(xr_train1)
xr_test_pca = pca.transform(xr_test1)
explained_variance = pca.explained_variance_ratio_
#%%
model=RandomForestClassifier(n_estimators=100, criterion='gini', random_state = 100,max_depth=6, min_samples_leaf=8)
#%%
model.fit(xr_train_pca,yr_train1)
#%%
RandomForestClassifier(max_depth=6, min_samples_leaf=8, random_state=100)
#%%
yr_predict_pca = model.predict(xr_test_pca)
#%%
model_score_r_pca = model.score(xr_test_pca, yr_test1)
#%%
print(model_score_r_pca)
print(metrics.classification_report(yr_test1, yr_predict_pca))
#%%
import pickle
#%%
filename = "model.sav"
#%%
pickle.dump(model_rf_smote, open(filename, 'wb'))
#%%
loaded_model = pickle.load(open(filename, 'rb'))
#%%
model_score_r1 = loaded_model.score(xr_test1, yr_test1)
#%%
model_score_r1
#%%
